"""
DVE Expert (Data Verification Expert) - è³‡æ–™æŸ¥æ ¸å°ˆå®¶
é·ç§»ç‰ˆ - ä¿æŒåŸæœ‰é‚è¼¯,èª¿æ•´è·¯å¾‘

ç‰¹è‰²:
1. Regex JSON æå– (é˜²æ­¢å­—ä¸²å…§æ‹¬è™Ÿå¹²æ“¾)
2. è‡ªå‹•æ¸…æ´— Hallucination
3. å‹•æ…‹å­˜æª” (ä¿®å¾©å¯«æ­»æ¬„ä½çš„ Bug)
4. MongoDB + RAG æ¯”å°æ­·å²è³‡æ–™
"""

import json
import torch
import logging
import re
from datetime import datetime
from transformers import TextStreamer
from peft import PeftModel

# å¾ä¸Šå±¤å°å…¥
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from config import (
    DVE_ADAPTER_PATH,
    DVE_PROMPT_TEMPLATE,
    DVE_INSTRUCTION,
    DEVICE,
    ENABLE_FINETUNED_MODELS
)
from services.rag_service import rag_engine
from experts.base import BaseExpert

logger = logging.getLogger(__name__)


class DVE_Expert(BaseExpert):
    """
    DVE: è³‡æ–™æŸ¥æ ¸å°ˆå®¶ (Ultimate Robust Version)
    
    è·è²¬:
    - æ¯”å°ä½¿ç”¨è€…å£è¿°è³‡æ–™èˆ‡æ­·å²ç´€éŒ„
    - æ¨™è¨˜é¢¨éšªç­‰ç´š (LOW/MEDIUM/HIGH)
    - è‡ªå‹•å°å­˜æœ¬æ¬¡ç”³è«‹è³‡æ–™
    """
    
    def __init__(self):
        """åˆå§‹åŒ– DVE Expert"""
        # åªåœ¨å•Ÿç”¨ fine-tuned models æ™‚æ‰åˆå§‹åŒ– LLM
        if ENABLE_FINETUNED_MODELS:
            super().__init__()
            logger.info("âœ… DVE Expert åˆå§‹åŒ–å®Œæˆ (å« Fine-tuned Model)")
        else:
            logger.warning("âš ï¸  DVE Expert: Fine-tuned Model æœªå•Ÿç”¨")
            self.llm = None
        
        logger.info("âœ… DVE Expert å°±ç·’")
    
    def process(self, task_data, history=[]):
        """
        è™•ç† DVE ä»»å‹™
        
        Args:
            task_data: {
                "user_query": "ä½¿ç”¨è€…å•é¡Œ",
                "profile_state": {...},
                "verification_status": "pending"
            }
            history: å°è©±æ­·å²
        
        Returns:
            {
                "expert": "DVE (é¢¨éšªç­‰ç´š)",
                "response": "å›è¦†å…§å®¹",
                "dve_raw_report": {...},
                "next_step": "ä¸‹ä¸€æ­¥å»ºè­°"
            }
        """
        
        query = task_data.get("user_query", "")
        profile = task_data.get("profile_state", {})
        
        logger.info(f"ğŸ“ DVE è™•ç†: user_id={profile.get('id', 'UNKNOWN')}")
        
        # === 1. æŠ€è¡“éšœç¤™æ””æˆª (Rule-based) ===
        tech_keywords = ["å‚³ä¸ä¸Š", "å¤±æ•—", "æ ¼å¼éŒ¯èª¤", "å¤ªæ…¢", "ç•¶æ©Ÿ", "ç„¡æ³•"]
        if any(k in query for k in tech_keywords):
            logger.info("ğŸ›¡ï¸  DVE åµæ¸¬åˆ°æŠ€è¡“å•é¡Œ")
            return {
                "expert": "DVE (Tech Support)",
                "mode": "tech_support",
                "response": "åµæ¸¬åˆ°æŠ€è¡“å•é¡Œã€‚è«‹ç¢ºèªåœ–ç‰‡æ ¼å¼ç‚º JPG/PNG ä¸”å°æ–¼ 5MBã€‚",
                "next_step": "ç­‰å¾…æŠ€è¡“æ’é™¤"
            }
        
        logger.info("ğŸ›¡ï¸  DVE å•Ÿå‹• AI æŸ¥æ ¸æ¨¡å¼ (Loading from MongoDB)...")
        
        # === 2. æº–å‚™ RAG è³‡æ–™ (Context) ===
        user_id = profile.get("id", "UNKNOWN")
        user_name = profile.get("name", "Guest")
        
        # å¾ MongoDB æ’ˆå–æ­·å²ç´€éŒ„
        history_records = rag_engine.get_user_history_by_id(user_id)
        
        rag_context = {}
        
        if history_records:
            logger.info(f"ğŸ” ç™¼ç¾ {len(history_records)} ç­†æ­·å²ç´€éŒ„")
            latest_record = history_records[-1]  # å–æœ€æ–°
            meta = latest_record.get("metadata", {})
            
            # ç›´æ¥å¾ Metadata å°æ‡‰åˆ° DVE éœ€è¦çš„ Key
            rag_context = {
                "æª”æ¡ˆä¸­ç´€éŒ„è·æ¥­": meta.get("hist_job", "ç„¡ç´€éŒ„"),
                "ä¸Šæ¬¡è²¸æ¬¾è³‡é‡‘ç”¨é€”": meta.get("hist_purpose", "ç„¡ç´€éŒ„"),
                "æª”æ¡ˆä¸­è¯çµ¡é›»è©±": meta.get("hist_phone", "ç„¡ç´€éŒ„"),
                "æ­·å²é•ç´„ç´€éŒ„": meta.get("default_record", "ç„¡"),
                "æª”æ¡ˆä¸­æœå‹™å…¬å¸åç¨±": meta.get("hist_company", "ç„¡ç´€éŒ„"),
                "æª”æ¡ˆä¸­å¹´è–ª/æœˆè–ª": str(meta.get("hist_income", "0")),
                "ä¿¡ç”¨å ±å‘ŠæŸ¥è©¢æ¬¡æ•¸": str(meta.get("inquiry_count", "0")),
                "åœ°å€è®Šå‹•æ¬¡æ•¸": str(meta.get("addr_change_count", "0"))
            }
        else:
            logger.warning("âš ï¸  æ–°ç”¨æˆ¶ (ç„¡æ­·å²ç´€éŒ„)")
            rag_context = {
                "æª”æ¡ˆä¸­ç´€éŒ„è·æ¥­": "ç„¡ç´€éŒ„ (æ–°æˆ¶)",
                "ä¸Šæ¬¡è²¸æ¬¾è³‡é‡‘ç”¨é€”": "ç„¡ç´€éŒ„",
                "æª”æ¡ˆä¸­è¯çµ¡é›»è©±": "ç„¡ç´€éŒ„",
                "æ­·å²é•ç´„ç´€éŒ„": "ç„¡",
                "æª”æ¡ˆä¸­æœå‹™å…¬å¸åç¨±": "ç„¡ç´€éŒ„",
                "æª”æ¡ˆä¸­å¹´è–ª/æœˆè–ª": "0",
                "ä¿¡ç”¨å ±å‘ŠæŸ¥è©¢æ¬¡æ•¸": "0",
                "åœ°å€è®Šå‹•æ¬¡æ•¸": "0"
            }
        
        # === 3. çµ„å»º Input JSON ===
        # æå–è®Šæ•¸ä»¥ä¾¿å¾ŒçºŒå­˜æª”
        q_job = profile.get("job", "å¾…æ¥­ä¸­")
        q_purpose = profile.get("purpose", "ä¸€èˆ¬é€±è½‰")
        q_phone = profile.get("phone", "09xx-xxx-xxx")
        q_company = profile.get("company", "æœªæä¾›")
        q_income = str(profile.get("income", "0"))
        
        dve_input_data = {
            "æ ¸å¿ƒè­˜åˆ¥è³‡è¨Š": {
                "ç”³è«‹äººå§“å": user_name,
                "èº«åˆ†è­‰å­—è™Ÿ": user_id
            },
            "æœ€æ–°å£è¿°è³‡è¨Š (Query) æ“·å–": {
                "è·æ¥­": q_job,
                "è³‡é‡‘ç”¨é€”": q_purpose,
                "è¯çµ¡é›»è©±": q_phone,
                "æœå‹™å…¬å¸åç¨±": q_company,
                "æœˆè–ª": q_income
            },
            "RAG æª¢ç´¢çš„æ­·å²æ•¸æ“š (Context) æ“·å–": rag_context
        }
        
        input_json_str = json.dumps(dve_input_data, ensure_ascii=False)
        
        logger.debug(f"ğŸ“ DVE Input JSON:\n{json.dumps(dve_input_data, indent=2, ensure_ascii=False)}")
        
        # === 4. å‘¼å« LLM é€²è¡Œé©—è­‰ ===
        if not ENABLE_FINETUNED_MODELS or self.llm is None:
            logger.warning("âš ï¸  Fine-tuned Model æœªå•Ÿç”¨,ä½¿ç”¨è¦å‰‡å¼é©—è­‰")
            return self._rule_based_verification(
                profile, rag_context, q_job, q_purpose, q_phone, q_company, q_income
            )
        
        try:
            report = self._ai_verification(
                input_json_str,
                user_name,
                user_id,
                q_job,
                q_purpose,
                q_phone,
                q_company,
                q_income,
                rag_context
            )
            
            return self._process_verification_result(
                report,
                user_name,
                user_id,
                q_job,
                q_purpose,
                q_phone,
                q_company,
                q_income,
                rag_context
            )
            
        except Exception as e:
            logger.error(f"âŒ DVE AI é©—è­‰å¤±æ•—: {e}", exc_info=True)
            
            # Fallback è¦å‰‡å¼é©—è­‰
            return self._rule_based_verification(
                profile, rag_context, q_job, q_purpose, q_phone, q_company, q_income
            )
    
    def _ai_verification(
        self,
        input_json_str,
        user_name,
        user_id,
        q_job,
        q_purpose,
        q_phone,
        q_company,
        q_income,
        rag_context
    ):
        """
        AI æ¨¡å‹é©—è­‰
        ä½¿ç”¨ Fine-tuned Model
        """
        
        logger.info("ğŸ¤– DVE AI é©—è­‰æ¨¡å¼ (Fine-tuned Model)")
        
        # è¼‰å…¥æ¨¡å‹
        model = PeftModel.from_pretrained(self.llm._base_model, DVE_ADAPTER_PATH)
        model.eval()
        
        tokenizer = self.llm._tokenizer
        
        # æ§‹å»º Prompt
        prompt = DVE_PROMPT_TEMPLATE.format(
            instruction=DVE_INSTRUCTION,
            input_text=input_json_str
        )
        
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # ç§»å‹•åˆ°æ­£ç¢ºè¨­å‚™
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Stream æ¨¡å¼ç”Ÿæˆ
        streamer = TextStreamer(tokenizer, skip_prompt=True)
        
        logger.info("ğŸŒŠ é–‹å§‹ç”Ÿæˆ DVE å ±å‘Š (Stream Mode)...")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                streamer=streamer,
                max_new_tokens=512,
                temperature=0.1,
                repetition_penalty=1.2,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç¢¼å®Œæ•´æ–‡å­—
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # è§£æ JSON
        report = self._parse_dve_output(full_text)
        
        return report
    
    def _parse_dve_output(self, full_text):
        """
        è§£æ DVE æ¨¡å‹è¼¸å‡º
        ä½¿ç”¨ Regex æå– JSON
        """
        
        try:
            # Step A: ç²—ç•¥åˆ‡å‰²
            if "<|end_of_text|>" in full_text:
                full_text = full_text.split("<|end_of_text|>")[0]
            
            if "### Output:" in full_text:
                generated_text = full_text.split("### Output:")[1].strip()
            else:
                generated_text = full_text
            
            # Step B: æ¸…æ´—å·²çŸ¥çš„æ€ªç•° Token
            generated_text = generated_text.replace("PortÃ¡ly", "")
            
            # Step C: JSON æå– (Regex å„ªå…ˆ)
            match = re.search(r"(\{.*\})", generated_text, re.DOTALL)
            
            json_str = ""
            if match:
                json_str = match.group(1)
            else:
                # Fallback: find/rfind
                start_idx = generated_text.find("{")
                end_idx = generated_text.rfind("}")
                if start_idx != -1 and end_idx != -1:
                    json_str = generated_text[start_idx:end_idx+1]
            
            if not json_str:
                raise ValueError("ç„¡æ³•æå– JSON çµæ§‹")
            
            # Step D: JSON è¼‰å…¥èˆ‡ä¿®å¾©
            try:
                report = json.loads(json_str)
            except json.JSONDecodeError:
                # å˜—è©¦ä¿®å¾©å¼•è™Ÿ
                if json_str.count('"') % 2 != 0:
                    json_str = json_str.replace('"}', '"}')
                report = json.loads(json_str)
            
            logger.info(f"âœ… DVE å ±å‘Šè§£ææˆåŠŸ: {str(report)[:100]}...")
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ DVE å ±å‘Šè§£æå¤±æ•—: {e}")
            raise
    
    def _process_verification_result(
        self,
        report,
        user_name,
        user_id,
        q_job,
        q_purpose,
        q_phone,
        q_company,
        q_income,
        rag_context
    ):
        """
        è™•ç†é©—è­‰çµæœä¸¦è‡ªå‹•å­˜æª”
        """
        
        # è®€å–çµæœ
        check_status = report.get("æ ¸å¯¦ç‹€æ…‹", "UNKNOWN")
        risk_level = report.get("é¢¨éšªæ¨™è¨˜", "MEDIUM")
        
        # å¼·åˆ¶é‚è¼¯: å¦‚æœæœ‰ MISMATCH_FOUND,é¢¨éšªä¸å¯èƒ½æ˜¯ LOW
        if check_status == "MISMATCH_FOUND" and risk_level == "LOW":
            risk_level = "MEDIUM"
            logger.warning("âš ï¸  å¼·åˆ¶ä¿®æ­£: MISMATCH_FOUND ä¸æ‡‰ç‚º LOW é¢¨éšª")
        
        # === è‡ªå‹•å­˜æª”æ©Ÿåˆ¶ ===
        logger.info(f"ğŸ’¾ æ­£åœ¨å°å­˜æœ¬æ¬¡ç”³è«‹è³‡æ–™è‡³ MongoDB ({user_name})...")
        
        archive_content = (
            f"ã€éŠ€è¡Œå…§éƒ¨å­˜æª”ã€‘\n"
            f"å­˜æª”æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"å®¢æˆ¶å§“å: {user_name} ({user_id})\n"
            f"è·æ¥­ç´€éŒ„: ä»»è·æ–¼ã€Œ{q_company}ã€,è·ç¨±ç‚ºã€Œ{q_job}ã€\n"
            f"è²¡å‹™ç´€éŒ„: å£è¿°æœˆè–ª {q_income} å…ƒ\n"
            f"æŸ¥æ ¸çµæœ: æœ¬æ¬¡ DVE æŸ¥æ ¸é¢¨éšªç‚º {risk_level}"
        )
        
        # Metadata å‹•æ…‹å¯«å…¥
        archive_meta = {
            "name": user_name,
            "hist_job": q_job,
            "hist_company": q_company,
            "hist_income": q_income,
            "hist_phone": q_phone,
            "hist_purpose": q_purpose,
            "default_record": "ç„¡",
            "inquiry_count": str(int(rag_context.get("ä¿¡ç”¨å ±å‘ŠæŸ¥è©¢æ¬¡æ•¸", "0")) + 1),
            "last_risk_level": risk_level,
            "check_status": check_status
        }
        
        # å¯«å…¥è³‡æ–™åº«
        rag_engine.add_document(user_id, archive_content, metadata=archive_meta)
        
        logger.info("âœ… è³‡æ–™å°å­˜å®Œæˆ!")
        
        # === æ±ºå®šå›è¦†èˆ‡ä¸‹ä¸€æ­¥ ===
        if risk_level == "LOW":
            user_res = "è³‡æ–™é©—è­‰ç„¡èª¤,æ­£åœ¨ç‚ºæ‚¨é€²è¡Œè©¦ç®—ã€‚"
            next_step = "TRANSFER_TO_FRE"
        elif risk_level == "HIGH":
            user_res = "ç³»çµ±åµæ¸¬åˆ°æ‚¨çš„è³‡æ–™èˆ‡ç´€éŒ„æœ‰å‡ºå…¥,è«‹èªªæ˜ç›®å‰ç‹€æ³ã€‚"
            next_step = "FORCE_LDE_CLARIFY"
        else:  # MEDIUM
            user_res = "è³‡æ–™å·²å—ç†,å°‡è½‰ç”±äººå·¥è¦†æ ¸ã€‚"
            next_step = "TRANSFER_TO_FRE"
        
        return {
            "expert": f"DVE ({risk_level})",
            "mode": "ai_verification",
            "response": user_res,
            "dve_raw_report": report,
            "next_step": next_step,
            "risk_level": risk_level,
            "check_status": check_status
        }
    
    def _rule_based_verification(
        self,
        profile,
        rag_context,
        q_job,
        q_purpose,
        q_phone,
        q_company,
        q_income
    ):
        """
        è¦å‰‡å¼é©—è­‰ (Fallback)
        ç•¶ AI æ¨¡å‹ä¸å¯ç”¨æ™‚ä½¿ç”¨
        """
        
        logger.info("ğŸ”§ DVE è¦å‰‡å¼é©—è­‰æ¨¡å¼ (Fallback)")
        
        # ç°¡å–®è¦å‰‡æ¯”å°
        mismatches = []
        
        # æ¯”å°è·æ¥­
        hist_job = rag_context.get("æª”æ¡ˆä¸­ç´€éŒ„è·æ¥­", "")
        if hist_job != "ç„¡ç´€éŒ„" and hist_job != "ç„¡ç´€éŒ„ (æ–°æˆ¶)" and hist_job != q_job:
            mismatches.append(f"è·æ¥­ä¸ç¬¦ (æ­·å²: {hist_job}, å£è¿°: {q_job})")
        
        # æ¯”å°æ”¶å…¥
        hist_income = rag_context.get("æª”æ¡ˆä¸­å¹´è–ª/æœˆè–ª", "0")
        if hist_income != "0" and abs(int(hist_income) - int(q_income)) > int(q_income) * 0.2:
            mismatches.append(f"æ”¶å…¥å·®ç•°éå¤§ (æ­·å²: {hist_income}, å£è¿°: {q_income})")
        
        # æ¯”å°é›»è©±
        hist_phone = rag_context.get("æª”æ¡ˆä¸­è¯çµ¡é›»è©±", "")
        if hist_phone != "ç„¡ç´€éŒ„" and hist_phone != q_phone:
            mismatches.append(f"é›»è©±ä¸ç¬¦ (æ­·å²: {hist_phone}, å£è¿°: {q_phone})")
        
        # åˆ¤æ–·é¢¨éšªç­‰ç´š
        if len(mismatches) >= 2:
            risk_level = "HIGH"
            user_res = "ç³»çµ±åµæ¸¬åˆ°æ‚¨çš„è³‡æ–™èˆ‡ç´€éŒ„æœ‰å¤šè™•ä¸ç¬¦,è«‹èªªæ˜ã€‚"
            next_step = "FORCE_LDE_CLARIFY"
        elif len(mismatches) == 1:
            risk_level = "MEDIUM"
            user_res = "è³‡æ–™å·²å—ç†,å°‡è½‰ç”±äººå·¥è¦†æ ¸ã€‚"
            next_step = "TRANSFER_TO_FRE"
        else:
            risk_level = "LOW"
            user_res = "è³‡æ–™é©—è­‰ç„¡èª¤,æ­£åœ¨ç‚ºæ‚¨é€²è¡Œè©¦ç®—ã€‚"
            next_step = "TRANSFER_TO_FRE"
        
        logger.info(f"ğŸ”§ è¦å‰‡å¼é©—è­‰çµæœ: {risk_level}, ä¸ç¬¦é …ç›®: {len(mismatches)}")
        
        return {
            "expert": f"DVE ({risk_level})",
            "mode": "rule_based",
            "response": user_res,
            "dve_raw_report": {
                "é¢¨éšªæ¨™è¨˜": risk_level,
                "æ ¸å¯¦ç‹€æ…‹": "CHECKED",
                "ä¸ç¬¦é …ç›®": mismatches
            },
            "next_step": next_step,
            "risk_level": risk_level
        }