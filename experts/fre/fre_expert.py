"""
FRE Expert (Financial Risk Expert) - æœ€çµ‚é¢¨æ§å°ˆå®¶

è² è²¬ï¼š
- ç¶œåˆè©•ä¼°ç”³è«‹äººçš„ä¿¡ç”¨èˆ‡è²¡å‹™ç‹€æ³
- ğŸ” ä½¿ç”¨ RAG æœå°‹ç›¸ä¼¼æ¡ˆä¾‹è¼”åŠ©æ±ºç­–
- ç”Ÿæˆæœ€çµ‚æ±ºç­– (æ ¸å‡†/æ‹’çµ•/è½‰ä»‹)
- æ‡‰ç”¨å®‰å…¨é–é˜²æ­¢é‚è¼¯éŒ¯èª¤
"""

import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Tuple

import sys
import os

# ç¢ºä¿å¯ä»¥æ­£ç¢º import å°ˆæ¡ˆæ¨¡çµ„
_project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

# å»¶é² import torch (å¯èƒ½ä¸å­˜åœ¨)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

from config import (
    FRE_ADAPTER_PATH,
    FRE_PROMPT_TEMPLATE,
    FRE_INSTRUCTION,
    DEVICE,
    ENABLE_FINETUNED_MODELS
)

# ä½¿ç”¨çµ•å°è·¯å¾‘ import BaseExpert
from experts.base import BaseExpert

# ğŸ” å°å…¥ RAG Service
from services.rag_service import rag_engine

logger = logging.getLogger(__name__)


class FREExpert(BaseExpert):
    """
    FRE: æœ€çµ‚é¢¨æ§å°ˆå®¶ (Streamer + Schema Matching + Safety Guard)
    """
    
    def __init__(self):
        """åˆå§‹åŒ– FRE Expert"""
        if ENABLE_FINETUNED_MODELS:
            super().__init__()
            logger.info("âœ… FRE Expert åˆå§‹åŒ–å®Œæˆ (å« Fine-tuned Model)")
        else:
            logger.warning("âš ï¸ FRE Expert: Fine-tuned Model æœªå•Ÿç”¨")
            self.llm = None
        
        logger.info("âœ… FRE Expert å°±ç·’")
    
    def process(self, task_data: Dict, history: List = None) -> Dict[str, Any]:
        """è™•ç†é¢¨æ§æ±ºç­–ä»»å‹™"""
        if history is None:
            history = []
            
        profile = task_data.get("profile_state", {})
        dve_result = task_data.get("dve_result", {})
        
        def safe_int(val, default=0):
            try:
                if val in [None, "null", "è³‡æ–™ä¸è¶³", "", "None"]:
                    return default
                return int(float(val))
            except (ValueError, TypeError):
                return default

        def safe_str(val, default="è³‡æ–™ä¸è¶³"):
            if val in [None, "null", "", "None"]:
                return default
            return str(val)

        p_income = safe_int(profile.get("income"))
        p_amount = safe_int(profile.get("amount"))
        p_job = safe_str(profile.get("job"))
        
        h_income = p_income if p_income > 0 else 60000 
        calc_base_income = p_income if p_income > 0 else h_income
        
        monthly_pay = int((p_amount * 1.03) / 84) if p_amount > 0 else 0
        dbr = (monthly_pay / calc_base_income * 100) if calc_base_income > 0 else 0
        
        credit_score = 700 if calc_base_income > 40000 else 600

        current_time = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        
        fre_input_data = {
            "caseId": f"CASE_{int(time.time())}",
            "creationDate": current_time,
            "scenarioType": "REAL_TIME_INFERENCE",
            "customerIdentity": {
                "èº«åˆ†è­‰å­—è™Ÿ": safe_str(profile.get("id"), "UNKNOWN"),
                "ç”³è«‹äººå§“å": safe_str(profile.get("name"), "Guest")
            },
            "applicationData": {
                "ç”³è«‹é‡‘é¡": p_amount,
                "ç”³è«‹ç”¨é€”_å®˜æ–¹": "é€±è½‰é‡‘"
            },
            "creditReportData": {
                "ç³»çµ±åŸå§‹ä¿¡ç”¨è©•åˆ†": credit_score,
                "ç¾æœ‰ç¸½è² å‚µé‡‘é¡": 0,
                "æ­·å²é•ç´„ç´€éŒ„": "ç„¡" if credit_score >= 650 else "æœ‰",
                "ä¿¡ç”¨å ±å‘ŠæŸ¥è©¢æ¬¡æ•¸_è¿‘3æœˆ": 1
            },
            "providedData": {
                "å£è¿°æœˆè–ª": profile.get("income"),
                "å£è¿°è·æ¥­": p_job,
                "å£è¿°å…¬å¸åç¨±": safe_str(profile.get("company")),
                "å£è¿°è¯çµ¡é›»è©±": "09xx-xxx-xxx",
                "å£è¿°è³‡é‡‘ç”¨é€”": "é€±è½‰é‡‘"
            },
            "historicalData": {
                "æ­·å²æœˆè–ª": h_income, 
                "æ­·å²è·æ¥­": "è³‡æ–™åº«ç´€éŒ„"
            },
            "system_hint": {
                "dve_risk_label": dve_result.get("risk_level", "LOW"),
                "calculated_dbr": f"{dbr:.1f}%"
            }
        }
        
        input_json_str = json.dumps(fre_input_data, ensure_ascii=False)
        logger.info(f"ğŸ’° FRE Input æ§‹å»ºå®Œæˆ (DBR: {dbr:.1f}%, Score: {credit_score})")

        if not ENABLE_FINETUNED_MODELS or self.llm is None:
            logger.warning("âš ï¸ Fine-tuned Model æœªå•Ÿç”¨ï¼Œä½¿ç”¨è¦å‰‡å¼æ±ºç­–")
            return self._rule_based_decision(
                p_income, p_job, p_amount, dbr, credit_score, dve_result,
                profile=profile
            )
        
        try:
            return self._ai_decision(
                input_json_str, p_income, p_job, p_amount, dbr, credit_score
            )
        except Exception as e:
            logger.error(f"âŒ FRE AI æ±ºç­–å¤±æ•—: {e}", exc_info=True)
            return self._rule_based_decision(
                p_income, p_job, p_amount, dbr, credit_score, dve_result,
                profile=profile
            )
    
    def _ai_decision(
        self,
        input_json_str: str,
        p_income: int,
        p_job: str,
        p_amount: int,
        dbr: float,
        credit_score: int
    ) -> Dict[str, Any]:
        """AI æ¨¡å‹æ±ºç­–"""
        from transformers import TextStreamer
        from peft import PeftModel
        
        logger.info("ğŸŒŠ é–‹å§‹ç”Ÿæˆæ±ºç­– (Stream Mode)...")
        
        streamer = TextStreamer(self.llm._tokenizer, skip_prompt=True)
        model = self.llm._base_model
        tokenizer = self.llm._tokenizer
        
        model = PeftModel.from_pretrained(model, FRE_ADAPTER_PATH)
        model.eval()

        prompt = FRE_PROMPT_TEMPLATE.format(
            instruction=FRE_INSTRUCTION,
            input_text=input_json_str
        )
        inputs = tokenizer(prompt, return_tensors="pt")
        
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                streamer=streamer,
                max_new_tokens=512,
                temperature=0.1,
                repetition_penalty=1.2,
                eos_token_id=tokenizer.eos_token_id
            )

        full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
        generated_text = self._parse_output(full_text)
        report = self._extract_json(generated_text)
        
        final_block = report.get("æœ€çµ‚æ±ºç­–å ±å‘Š", report)
        decision = final_block.get("æœ€çµ‚æ±ºç­–") or report.get("æœ€çµ‚æ±ºç­–") or "è½‰ä»‹å¯©æ ¸_ESCALATE"
        
        decision, override_msg = self._apply_safety_guard(
            decision, p_income, p_job, dbr, credit_score
        )
        
        user_msg, next_step = self._generate_response(decision, credit_score, p_amount)

        return {
            "expert": f"FRE ({decision}) {override_msg}",
            "mode": "ai_decision",
            "response": user_msg,
            "fre_raw_report": report,
            "financial_metrics": {"dbr": dbr, "score": credit_score},
            "next_step": next_step
        }

    def _parse_output(self, full_text: str) -> str:
        if "<|end_of_text|>" in full_text:
            full_text = full_text.split("<|end_of_text|>")[0]
        if "### Output:" in full_text:
            return full_text.split("### Output:")[1].strip()
        return full_text

    def _extract_json(self, text: str) -> Dict:
        if "{" not in text:
            return {}
        try:
            json_str = text[text.find("{"):text.rfind("}")+1]
            return json.loads(json_str)
        except (json.JSONDecodeError, ValueError):
            return {}

    def _apply_safety_guard(
        self, decision: str, p_income: int, p_job: str, dbr: float, credit_score: int
    ) -> Tuple[str, str]:
        override_msg = ""
        
        missing_critical = (p_income == 0) or (p_job == "è³‡æ–™ä¸è¶³")
        if missing_critical and ("PASS" in decision or "æ ¸å‡†" in decision):
            decision = "è½‰ä»‹å¯©æ ¸_ESCALATE"
            override_msg = "(ç³»çµ±ä¿®æ­£: é—œéµè³‡æ–™ç¼ºå¤±)"
            logger.warning("âš ï¸ FRE Guard: æ””æˆªåˆ°è³‡æ–™ç¼ºå¤±")

        elif dbr > 60 and ("PASS" in decision or "æ ¸å‡†" in decision):
            decision = "æ‹’çµ•_REJECT"
            override_msg = f"(ç³»çµ±ä¿®æ­£: DBR {dbr:.1f}% éé«˜)"
            logger.warning("âš ï¸ FRE Guard: æ””æˆªåˆ°é«˜è² å‚µæ¯”")
        
        elif credit_score < 650 and ("PASS" in decision or "æ ¸å‡†" in decision):
            decision = "æ‹’çµ•_REJECT"
            override_msg = "(ç³»çµ±ä¿®æ­£: ä¿¡ç”¨åˆ†ä¸è¶³)"
            logger.warning("âš ï¸ FRE Guard: æ””æˆªåˆ°ä½ä¿¡ç”¨åˆ†")

        return decision, override_msg

    def _generate_response(
        self, decision: str, credit_score: int, p_amount: int
    ) -> Tuple[str, str]:
        if "PASS" in decision or "æ ¸å‡†" in decision:
            user_msg = f"æ­å–œï¼æ‚¨çš„ä¿¡ç”¨è©•åˆ† ({credit_score}åˆ†) ç¬¦åˆæ¨™æº–ã€‚\nåˆå¯©é¡åº¦: {p_amount:,} å…ƒ"
            next_step = "CASE_CLOSED_SUCCESS"
        elif "REJECT" in decision or "æ‹’çµ•" in decision:
            user_msg = "æ„Ÿè¬ç”³è«‹ã€‚ç¶“ç¶œåˆè©•ä¼°ï¼Œæš«æ™‚ç„¡æ³•æ ¸è²¸ã€‚"
            next_step = "CASE_CLOSED_REJECT"
        else:
            user_msg = "ç”³è«‹å·²å—ç†ï¼Œå°‡è½‰ç”±äººå·¥è¦†æ ¸ã€‚"
            next_step = "HUMAN_HANDOVER"
        return user_msg, next_step
    
    def _rule_based_decision(
        self, p_income: int, p_job: str, p_amount: int,
        dbr: float, credit_score: int, dve_result: Dict,
        profile: Dict = None
    ) -> Dict[str, Any]:
        """
        è¦å‰‡å¼æ±ºç­– (Fallback)
        
        ğŸ” åŠ å…¥ RAG: æœå°‹ case_library ä¸­çš„ç›¸ä¼¼æ¡ˆä¾‹ä½œç‚ºåƒè€ƒ
        """
        logger.info("ğŸ”§ FRE è¦å‰‡å¼æ±ºç­–æ¨¡å¼ (Fallback) + RAG")
        
        dve_risk = dve_result.get("risk_level", "MEDIUM")
        
        # === ğŸ” RAG: æœå°‹ç›¸ä¼¼æ¡ˆä¾‹ ===
        rag_reference = None
        if profile:
            try:
                rag_reference = rag_engine.get_reference_for_decision(
                    profile=profile,
                    dve_risk_level=dve_risk,
                    top_k=3
                )
                if rag_reference.get("similar_cases"):
                    logger.info(f"ğŸ“š RAG: æ‰¾åˆ° {len(rag_reference['similar_cases'])} ç­†ç›¸ä¼¼æ¡ˆä¾‹")
                    logger.info(f"ğŸ“š RAG å»ºè­°: {rag_reference.get('recommendation')}")
            except Exception as e:
                logger.warning(f"âš ï¸ RAG æŸ¥è©¢å¤±æ•—: {e}")
        
        # === æ±ºç­–é‚è¼¯ ===
        # å„ªå…ˆä½¿ç”¨ç¡¬è¦å‰‡
        if dve_risk == "HIGH" or credit_score < 650 or dbr > 45:
            decision = "æ‹’çµ•_REJECT"
            user_msg = "æ„Ÿè¬ç”³è«‹ã€‚ç¶“ç¶œåˆè©•ä¼°ï¼Œæš«æ™‚ç„¡æ³•æ ¸è²¸ã€‚"
            next_step = "CASE_CLOSED_REJECT"
        elif dve_risk == "MEDIUM" or dbr > 30:
            decision = "è½‰ä»‹å¯©æ ¸_ESCALATE"
            user_msg = "ç”³è«‹å·²å—ç†ï¼Œå°‡è½‰ç”±äººå·¥è¦†æ ¸ã€‚"
            next_step = "HUMAN_HANDOVER"
        else:
            # å¯åƒè€ƒ RAG çµæœå¾®èª¿
            if rag_reference and rag_reference.get("approval_rate") is not None:
                approval_rate = rag_reference["approval_rate"]
                if approval_rate < 0.3:
                    # ç›¸ä¼¼æ¡ˆä¾‹æ ¸å‡†ç‡å¾ˆä½ï¼Œè¬¹æ…è™•ç†
                    decision = "è½‰ä»‹å¯©æ ¸_ESCALATE"
                    user_msg = "ç”³è«‹å·²å—ç†ï¼Œå°‡è½‰ç”±äººå·¥è¦†æ ¸ã€‚"
                    next_step = "HUMAN_HANDOVER"
                    logger.info(f"ğŸ“š RAG å½±éŸ¿æ±ºç­–: ç›¸ä¼¼æ¡ˆä¾‹æ ¸å‡†ç‡åƒ… {approval_rate:.0%}ï¼Œè½‰äººå·¥")
                else:
                    decision = "æ ¸å‡†_PASS"
                    user_msg = f"æ­å–œï¼æ‚¨çš„ä¿¡ç”¨è©•åˆ† ({credit_score}åˆ†) ç¬¦åˆæ¨™æº–ã€‚\nåˆå¯©é¡åº¦: {p_amount:,} å…ƒ"
                    next_step = "CASE_CLOSED_SUCCESS"
            else:
                decision = "æ ¸å‡†_PASS"
                user_msg = f"æ­å–œï¼æ‚¨çš„ä¿¡ç”¨è©•åˆ† ({credit_score}åˆ†) ç¬¦åˆæ¨™æº–ã€‚\nåˆå¯©é¡åº¦: {p_amount:,} å…ƒ"
                next_step = "CASE_CLOSED_SUCCESS"
        
        logger.info(f"ğŸ”§ è¦å‰‡å¼æ±ºç­–çµæœ: {decision}")
        
        result = {
            "expert": f"FRE ({decision})",
            "mode": "rule_based",
            "response": user_msg,
            "fre_raw_report": {
                "æ±ºç­–": decision,
                "DBR": f"{dbr:.1f}%",
                "ä¿¡ç”¨è©•åˆ†": credit_score,
                "DVEé¢¨éšª": dve_risk
            },
            "financial_metrics": {"dbr": dbr, "score": credit_score},
            "next_step": next_step
        }
        
        # åŠ å…¥ RAG åƒè€ƒè³‡è¨Š
        if rag_reference:
            result["rag_reference"] = {
                "similar_cases_count": len(rag_reference.get("similar_cases", [])),
                "approval_rate": rag_reference.get("approval_rate"),
                "avg_approved_amount": rag_reference.get("avg_approved_amount"),
                "recommendation": rag_reference.get("recommendation")
            }
        
        return result
