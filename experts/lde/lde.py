"""
LDE Expert (Loan Desk Expert) - è²¸æ¬¾å¾µå¯©å°ˆå®¶
æ•´åˆç‰ˆ - ä¿ç•™åŸæ¶æ§‹ + Gemini æ›¿ä»£ OpenAI

å…©ç¨®æ¨¡å¼:
- Mode A (Consult): ä½¿ç”¨ Fine-tuned Model é€²è¡Œå°ˆæ¥­è«®è©¢
- Mode B (Guide): ä½¿ç”¨ Gemini API é€²è¡Œè³‡æ–™è£œå¼·èˆ‡å¼•å°
"""

import json
import logging
from google import genai

# å¾ä¸Šå±¤å°å…¥
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from config import (
    GEMINI_API_KEY,
    GEMINI_MODEL_NAME,
    LDE_ADAPTER_PATH,
    LDE_SYSTEM_INSTRUCTION,
    LDE_PROMPT_TEMPLATE,
    ENABLE_FINETUNED_MODELS
)
from llm_utils import LocalLLMManager
from experts.base import BaseExpert

logger = logging.getLogger(__name__)

# åˆå§‹åŒ– Gemini Client (æ¨¡çµ„å±¤ç´š,åªåˆå§‹åŒ–ä¸€æ¬¡)
gemini_client = genai.Client(api_key=GEMINI_API_KEY)


class LDE_Expert(BaseExpert):
    """
    LDE: è²¸æ¬¾å¾µå¯©å°ˆå®¶ (Loan Desk Expert)
    Mode A: Local Fine-tuned Model (è«®è©¢)
    Mode B: Gemini API (è³‡æ–™æŠ½å–èˆ‡å¼•å°)
    """
    
    def __init__(self):
        """åˆå§‹åŒ– LDE Expert"""
        # åªåœ¨å•Ÿç”¨ fine-tuned models æ™‚æ‰åˆå§‹åŒ– LLM
        if ENABLE_FINETUNED_MODELS:
            super().__init__()  # ç¹¼æ‰¿ BaseExpert,æœƒåˆå§‹åŒ– self.llm
            logger.info("âœ… LDE Expert åˆå§‹åŒ–å®Œæˆ (å« Fine-tuned Model)")
        else:
            logger.info("â„¹ï¸  LDE Expert åˆå§‹åŒ– (åƒ… Gemini æ¨¡å¼)")
            self.llm = None
        
        logger.info("âœ… LDE Expert å°±ç·’")
    
    def process(self, task_data, history=[]):
        """
        è™•ç† LDE ä»»å‹™
        
        Args:
            task_data: {
                "user_query": "ä½¿ç”¨è€…å•é¡Œ",
                "profile_state": {...},
                "verification_status": "unknown|pending|verified|mismatch"
            }
            history: å°è©±æ­·å²
        
        Returns:
            {
                "expert": "LDE (Consult)" or "LDE (Guide)",
                "response": "å›è¦†å…§å®¹",
                "updated_profile": {...} or None,
                "next_step": "ä¸‹ä¸€æ­¥å»ºè­°"
            }
        """
        
        query = task_data.get("user_query", "")
        profile = task_data.get("profile_state", {})
        verification_status = task_data.get("verification_status", "unknown")
        
        logger.info(f"ğŸ“ LDE è™•ç†: query='{query[:50]}...', status={verification_status}")
        
        # === æ±ºå®šæ¨¡å¼ ===
        mode = self._decide_mode(query, profile, verification_status)
        
        logger.info(f"ğŸ¯ é¸æ“‡æ¨¡å¼: {mode}")
        
        # === Mode A: è«®è©¢æ¨¡å¼ ===
        if mode == "consult":
            return self._consult_mode(query, profile)
        
        # === Mode B: å¼•å°æ¨¡å¼ ===
        else:
            return self._guide_mode(query, profile, history)
    
    def _decide_mode(self, query, profile, verification_status):
        """
        æ±ºå®šä½¿ç”¨å“ªç¨®æ¨¡å¼
        
        é‚è¼¯:
        1. è³‡æ–™å¾ˆå°‘ (â‰¤2 å€‹æ¬„ä½) + è«®è©¢å•é¡Œ â†’ Consult
        2. è³‡æ–™ä¸å®Œæ•´ (unknown/pending) â†’ Guide
        3. è³‡æ–™æœ‰å•é¡Œ (mismatch) â†’ Guide
        4. è³‡æ–™å®Œæ•´ (verified) + å•å•é¡Œ â†’ Consult
        
        Returns:
            "consult" or "guide"
        """
        
        # è¨ˆç®—å·²å¡«å¯«æ¬„ä½æ•¸
        filled_count = sum(1 for v in profile.values() if v is not None)
        
        # è«®è©¢é—œéµå­—
        consult_keywords = [
            "å¤šå°‘", "åˆ©ç‡", "ä»€éº¼", "è³‡æ ¼", "å¯ä»¥å—",
            "è©¦ç®—", "å¥½é", "æ¨è–¦", "æ€éº¼", "å¦‚ä½•",
            "æ¢ä»¶", "å¯©æ ¸", "æœŸé™", "è²»ç”¨", "åˆ’ç®—"
        ]
        
        is_consult_question = any(kw in query for kw in consult_keywords)
        
        # === æ±ºç­–é‚è¼¯ ===
        
        # æƒ…æ³ 1: è³‡æ–™å¾ˆå°‘ (â‰¤2) + è«®è©¢å•é¡Œ â†’ Consult
        if filled_count <= 2 and is_consult_question:
            logger.debug("æ±ºç­–: è³‡æ–™å°‘ + è«®è©¢å•é¡Œ â†’ Consult")
            return "consult"
        
        # æƒ…æ³ 2: ç‹€æ…‹æ˜¯ unknown/pending/mismatch â†’ Guide (éœ€è¦è£œè³‡æ–™)
        if verification_status in ["unknown", "pending", "mismatch"]:
            logger.debug(f"æ±ºç­–: ç‹€æ…‹={verification_status} â†’ Guide")
            return "guide"
        
        # æƒ…æ³ 3: è³‡æ–™å·²é©—è­‰ (verified) + è«®è©¢å•é¡Œ â†’ Consult
        if verification_status == "verified" and is_consult_question:
            logger.debug("æ±ºç­–: å·²é©—è­‰ + è«®è©¢å•é¡Œ â†’ Consult")
            return "consult"
        
        # é è¨­: Guide Mode
        logger.debug("æ±ºç­–: é è¨­ â†’ Guide")
        return "guide"
    
    def _consult_mode(self, query, profile):
        """
        Mode A: è«®è©¢æ¨¡å¼
        ä½¿ç”¨ Fine-tuned Local Model å›ç­”å°ˆæ¥­å•é¡Œ
        """
        
        logger.info("ğŸ¤– LDE Mode A (Consult): Local Fine-tuned Model")
        
        try:
            # === æ§‹å»º Prompt ===
            # å¦‚æœæœ‰å®¢æˆ¶è³‡è¨Š,åŠ å…¥ context
            context = ""
            if profile:
                filled = {k: v for k, v in profile.items() if v is not None}
                if filled:
                    context = f"ã€å®¢æˆ¶è³‡è¨Šã€‘\n{json.dumps(filled, ensure_ascii=False)}\n\n"
            
            # æ§‹å»ºå®Œæ•´è¼¸å…¥
            input_text = f"{context}ã€å®¢æˆ¶å•é¡Œã€‘\n{query}"
            
            # === å‘¼å« Local LLM ===
            ai_response = self.llm.get_expert_response(
                adapter_path=LDE_ADAPTER_PATH,
                instruction=LDE_SYSTEM_INSTRUCTION,
                user_input=input_text,
                max_new_tokens=256,
                temperature=0.3,
                top_p=0.9,
                template=LDE_PROMPT_TEMPLATE
            )
            
            logger.info(f"âœ… Local Model å›è¦†: {ai_response[:100]}...")
            
            return {
                "expert": "LDE (Consult)",
                "mode": "consult",
                "response": ai_response,
                "updated_profile": None,
                "next_step": "ç­‰å¾…å®¢æˆ¶å¾ŒçºŒæ„é¡˜"
            }
            
        except Exception as e:
            logger.error(f"âŒ Consult Mode å¤±æ•—: {e}", exc_info=True)
            
            # Fallback: ä½¿ç”¨ Gemini
            logger.warning("âš ï¸  é™ç´šä½¿ç”¨ Gemini é€²è¡Œè«®è©¢")
            return self._consult_with_gemini(query, profile)
    
    def _consult_with_gemini(self, query, profile):
        """
        ä½¿ç”¨ Gemini é€²è¡Œè«®è©¢ (Fallback)
        """
        
        logger.info("ğŸ¤– ä½¿ç”¨ Gemini é€²è¡Œè«®è©¢ (Fallback)")
        
        # æ§‹å»º context
        context = ""
        if profile:
            filled = {k: v for k, v in profile.items() if v is not None}
            if filled:
                context = f"\nã€å®¢æˆ¶å·²æä¾›è³‡è¨Šã€‘\n{json.dumps(filled, ensure_ascii=False)}"
        
        # ä½¿ç”¨è¨“ç·´æ™‚çš„æŒ‡ä»¤
        prompt = f"""{LDE_SYSTEM_INSTRUCTION}
{context}

ã€å®¢æˆ¶å•é¡Œã€‘
{query}

è«‹ç”¨å°ˆæ¥­ã€ä¸­ç«‹çš„èªæ°£å›ç­”å®¢æˆ¶çš„å•é¡Œã€‚"""
        
        try:
            response = gemini_client.models.generate_content(
                model=GEMINI_MODEL_NAME,
                contents=prompt
            )
            
            ai_response = response.text.strip()
            
            logger.info(f"âœ… Gemini å›è¦†: {ai_response[:100]}...")
            
            return {
                "expert": "LDE (Consult via Gemini)",
                "mode": "consult",
                "response": ai_response,
                "updated_profile": None,
                "next_step": "ç­‰å¾…å®¢æˆ¶å¾ŒçºŒæ„é¡˜"
            }
            
        except Exception as e:
            logger.error(f"âŒ Gemini è«®è©¢å¤±æ•—: {e}", exc_info=True)
            
            return {
                "expert": "LDE (Consult)",
                "mode": "consult",
                "response": "æŠ±æ­‰,ç³»çµ±ç›®å‰ç¹å¿™,è«‹ç¨å¾Œå†è©¦ã€‚",
                "updated_profile": None,
                "next_step": "ç³»çµ±éŒ¯èª¤"
            }
    
    def _guide_mode(self, query, profile, history):
        """
        Mode B: å¼•å°æ¨¡å¼
        ä½¿ç”¨ Gemini é€²è¡Œè³‡æ–™æŠ½å–èˆ‡å¼•å°
        """
        
        logger.info("ğŸ¤– LDE Mode B (Guide): Gemini Extract & Guide")
        
        # === å‘¼å« Gemini é€²è¡ŒæŠ½å– ===
        extraction_result = self._gemini_extract(query, profile, history)
        
        # === æ›´æ–° Profile ===
        updated_profile = extraction_result.get("updated_profile", {})
        current_full_profile = profile.copy()
        
        if updated_profile:
            current_full_profile.update(updated_profile)
            logger.info(f"ğŸ“ æŠ½å–åˆ°: {updated_profile}")
        
        # === æª¢æŸ¥ç¼ºå°‘çš„æ¬„ä½ ===
        # æ³¨æ„: é€™è£¡è¦ç”¨ MoE è¨“ç·´æ™‚çš„æ¬„ä½å (purpose è€Œé loan_purpose)
        required = ["name", "id", "job", "income", "purpose", "amount"]
        missing = [k for k in required if not current_full_profile.get(k)]
        
        # === ç”Ÿæˆå›è¦† ===
        response_text = extraction_result.get("reply_to_user")
        
        if not response_text:
            # å¦‚æœ Gemini æ²’æœ‰ç”Ÿæˆå›è¦†,ä½¿ç”¨é è¨­é‚è¼¯
            if missing:
                # æ˜ å°„å›å°è©±æ™‚çš„æ¬„ä½å
                field_name_map = {
                    "name": "å§“å",
                    "id": "èº«åˆ†è­‰å­—è™Ÿ",
                    "job": "è·æ¥­",
                    "income": "æœˆæ”¶å…¥",
                    "purpose": "è²¸æ¬¾ç”¨é€”",
                    "amount": "è²¸æ¬¾é‡‘é¡"
                }
                next_field = field_name_map.get(missing[0], missing[0])
                response_text = f"æ”¶åˆ°ã€‚é‚„éœ€è¦è«‹å•æ‚¨çš„{next_field}æ˜¯?"
            else:
                response_text = "æ„Ÿè¬æ‚¨æä¾›å®Œæ•´è³‡è¨Š!æˆ‘å€‘å°‡ç‚ºæ‚¨é€²è¡Œå¯©æ ¸ã€‚"
        
        # === æ±ºå®šä¸‹ä¸€æ­¥ ===
        next_step = "ç­‰å¾…è£œä»¶" if missing else "è³‡æ–™å®Œæ•´,å¾…é©—è­‰"
        
        return {
            "expert": "LDE (Guide)",
            "mode": "guide",
            "response": response_text,
            "updated_profile": updated_profile if updated_profile else None,
            "next_step": next_step
        }
    
    def _gemini_extract(self, query, current_profile, history):
        """
        ä½¿ç”¨ Gemini API é€²è¡Œè³‡æ–™æŠ½å–
        æ›¿ä»£åŸæœ¬çš„ OpenAI
        
        Args:
            query: ä½¿ç”¨è€…ç•¶å‰è¼¸å…¥
            current_profile: ç›®å‰çš„ profile ç‹€æ…‹
            history: å°è©±æ­·å²
        
        Returns:
            {
                "updated_profile": {...},
                "reply_to_user": "..."
            }
        """
        
        # === æ§‹å»ºå°è©±æ­·å² ===
        history_text = ""
        if history:
            for msg in history[-5:]:  # åªå–æœ€è¿‘ 5 è¼ª
                role = msg.get("role", "user")
                content = msg.get("content", "")
                role_name = "User" if role == "user" else "Assistant"
                history_text += f"{role_name}: {content}\n"
        
        # === æ§‹å»º Prompt ===
        system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è²¸æ¬¾ç”³è«‹å¼•å°æ©Ÿå™¨äººã€‚

ã€å°è©±æ­·å²ã€‘
{history_text if history_text else "(ç„¡æ­·å²)"}

ã€ç•¶å‰è³‡æ–™ç‹€æ…‹ã€‘
{json.dumps(current_profile, ensure_ascii=False, indent=2)}

ã€ç•¶å‰ä½¿ç”¨è€…è¼¸å…¥ã€‘
User: {query}

ã€ä»»å‹™ã€‘
1. å¾ä½¿ç”¨è€…è¼¸å…¥ä¸­æŠ½å–ä»»ä½•å¯ç”¨çš„æ¬„ä½è³‡è¨Š
2. ç”Ÿæˆå‹å–„ã€å°ˆæ¥­çš„å›è¦†,å¼•å°ä½¿ç”¨è€…æä¾›ç¼ºå°‘çš„è³‡è¨Š

ç›®æ¨™æ¬„ä½: name (å§“å), id (èº«åˆ†è­‰), job (è·æ¥­), income (æœˆæ”¶å…¥), purpose (è²¸æ¬¾ç”¨é€”), amount (è²¸æ¬¾é‡‘é¡)

æ³¨æ„äº‹é …:
- é‡‘é¡è«‹è‡ªå‹•è½‰æ› (ä¾‹å¦‚ "5è¬" â†’ 50000)
- é›»è©±è™Ÿç¢¼å»é™¤ç©ºæ ¼å’Œç ´æŠ˜è™Ÿ
- å›è¦†è¦ç°¡æ½”ã€å°ˆæ¥­,ä¸è¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ

ã€è¼¸å‡ºæ ¼å¼ã€‘
è«‹è¼¸å‡ºç´” JSON,æ ¼å¼:
{{
  "updated_profile": {{
    "æ¬„ä½å": "æŠ½å–åˆ°çš„å€¼"
  }},
  "reply_to_user": "å‹å–„çš„å›è¦†æ–‡å­—"
}}

ç¯„ä¾‹ 1:
è¼¸å…¥: "æˆ‘å«ç‹å°æ˜,æœˆè–ªå¤§æ¦‚5è¬"
è¼¸å‡º: {{"updated_profile": {{"name": "ç‹å°æ˜", "income": 50000}}, "reply_to_user": "ç‹å°æ˜å…ˆç”Ÿæ‚¨å¥½,å·²è¨˜éŒ„æ‚¨çš„æœˆæ”¶å…¥ç‚º5è¬å…ƒã€‚è«‹å•æ‚¨çš„èº«åˆ†è­‰å­—è™Ÿæ˜¯?"}}

ç¯„ä¾‹ 2:
è¼¸å…¥: "æƒ³å€Ÿ60è¬è²·è»Š"
è¼¸å‡º: {{"updated_profile": {{"amount": 600000, "purpose": "è³¼è»Š"}}, "reply_to_user": "äº†è§£,è²¸æ¬¾é‡‘é¡60è¬å…ƒç”¨æ–¼è³¼è»Šå·²è¨˜éŒ„ã€‚è«‹å•æ‚¨çš„è·æ¥­æ˜¯?"}}

ç¾åœ¨è«‹é–‹å§‹è™•ç†:"""
        
        try:
            # === å‘¼å« Gemini ===
            response = gemini_client.models.generate_content(
                model=GEMINI_MODEL_NAME,
                contents=system_prompt
            )
            
            raw_output = response.text.strip()
            logger.debug(f"Gemini åŸå§‹è¼¸å‡º: {raw_output}")
            
            # === è§£æ JSON ===
            import re
            
            # æ¸…ç† markdown code block
            clean_output = re.sub(r'```json\s*', '', raw_output)
            clean_output = re.sub(r'```\s*', '', clean_output)
            
            result = json.loads(clean_output)
            
            logger.info(f"âœ… Gemini æŠ½å–æˆåŠŸ")
            
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON è§£æå¤±æ•—: {e}\nåŸå§‹è¼¸å‡º: {raw_output}")
            
            # Fallback
            return {
                "updated_profile": {},
                "reply_to_user": "æ”¶åˆ°æ‚¨çš„è¨Šæ¯ã€‚è«‹å•æ‚¨çš„å§“åæ˜¯?"
            }
        
        except Exception as e:
            logger.error(f"âŒ Gemini æŠ½å–å¤±æ•—: {e}", exc_info=True)
            
            # Fallback
            return {
                "updated_profile": {},
                "reply_to_user": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦ã€‚"
            }