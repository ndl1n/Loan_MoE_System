"""
Local LLM Manager - æœ¬åœ°æ¨¡å‹ç®¡ç†å™¨
ä½¿ç”¨ Singleton æ¨¡å¼ç®¡ç†æ‰€æœ‰å°ˆå®¶å…±ç”¨çš„ Base Model
"""

import torch
import os
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

from config import BASE_MODEL_PATH, DEVICE, ENABLE_FINETUNED_MODELS

logger = logging.getLogger(__name__)


class LocalLLMManager:
    """
    æœ¬åœ° LLM ç®¡ç†å™¨ (Singleton)
    
    è·è²¬:
    - è¼‰å…¥ä¸¦ç®¡ç† Base Model
    - å‹•æ…‹åˆ‡æ›ä¸åŒçš„ LoRA Adapter
    - åŸ·è¡Œæ¨ç†
    """
    
    _instance = None
    
    @classmethod
    def get_instance(cls):
        """å–å¾— Singleton å¯¦ä¾‹"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def __init__(self):
        """
        åˆå§‹åŒ– Base Model
        åªæœƒåŸ·è¡Œä¸€æ¬¡ (Singleton)
        """
        
        if LocalLLMManager._instance is not None:
            raise RuntimeError("è«‹ä½¿ç”¨ get_instance() å–å¾—å¯¦ä¾‹")
        
        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ Fine-tuned Models
        if not ENABLE_FINETUNED_MODELS:
            logger.warning("âš ï¸  Fine-tuned Models æœªå•Ÿç”¨ï¼ŒLocalLLMManager å°‡ä¸è¼‰å…¥æ¨¡å‹")
            self._tokenizer = None
            self._base_model = None
            self._loaded_adapters = {}
            self.terminators = []
            return
        
        logger.info(f"ğŸ”„ [LocalLLM] è¼‰å…¥ Base Model: {BASE_MODEL_PATH}...")
        
        # === é…ç½® 4-bit é‡åŒ– ===
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        
        # === è¼‰å…¥ Tokenizer ===
        self._tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL_PATH,
            trust_remote_code=True
        )
        
        # === è¼‰å…¥ Base Model ===
        self._base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            quantization_config=bnb_config,
            device_map="auto",  # è‡ªå‹•åˆ†é…åˆ° GPU
            trust_remote_code=True
        )
        
        # === è¨­å®šçµ‚æ­¢ç¬¦è™Ÿ ===
        self.terminators = [
            self._tokenizer.eos_token_id,
            self._tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        
        logger.info("âœ… Base Model è¼‰å…¥å®Œæˆ")
    
    def get_expert_response(
        self,
        adapter_path: str,
        instruction: str,
        user_input: str,
        max_new_tokens: int = 256,
        temperature: float = 0.3,
        top_p: float = 0.9,
        template: str = None
    ) -> str:
        """
        ä½¿ç”¨æŒ‡å®šçš„ Adapter é€²è¡Œæ¨ç†
        
        Args:
            adapter_path: LoRA Adapter è·¯å¾‘
            instruction: ç³»çµ±æŒ‡ä»¤
            user_input: ä½¿ç”¨è€…è¼¸å…¥
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•·åº¦
            temperature: æº«åº¦åƒæ•¸
            top_p: Top-p æ¡æ¨£
            template: Prompt Template (å¯é¸)
        
        Returns:
            ç”Ÿæˆçš„æ–‡å­—
        """
        
        # === 1. æª¢æŸ¥ Adapter æ˜¯å¦å­˜åœ¨ ===
        adapter_file = os.path.join(adapter_path, "adapter_model.safetensors")
        
        if not os.path.exists(adapter_file):
            logger.error(f"âŒ æ‰¾ä¸åˆ° Adapter: {adapter_file}")
            return "ç³»çµ±éŒ¯èª¤: æ‰¾ä¸åˆ°æ¨¡å‹æ¬Šé‡æª”"
        
        logger.debug(f"ğŸ“‚ è¼‰å…¥ Adapter: {adapter_path}")
        
        # === 2. æ›è¼‰ Adapter ===
        try:
            model = PeftModel.from_pretrained(self._base_model, adapter_path)
            model.eval()
        except Exception as e:
            logger.error(f"âŒ Adapter è¼‰å…¥å¤±æ•—: {e}", exc_info=True)
            return f"ç³»çµ±éŒ¯èª¤: æ¨¡å‹è¼‰å…¥å¤±æ•— ({str(e)})"
        
        # === 3. æ ¼å¼åŒ–è¼¸å…¥ ===
        if template:
            # ä½¿ç”¨æä¾›çš„ template
            # æ³¨æ„: template æ‡‰è©²æœ‰ {instruction} å’Œ {input_text} å…©å€‹ä½”ä½ç¬¦
            try:
                formatted_prompt = template.format(
                    instruction=instruction,
                    input_text=user_input
                )
            except KeyError:
                # å¦‚æœ template æ ¼å¼ä¸å°,ä½¿ç”¨é è¨­æ ¼å¼
                logger.warning("âš ï¸  Template æ ¼å¼éŒ¯èª¤,ä½¿ç”¨é è¨­æ ¼å¼")
                formatted_prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{user_input}\n\n### Output:\n"
        else:
            # é è¨­ Alpaca æ ¼å¼
            formatted_prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{user_input}\n\n### Output:\n"
        
        logger.debug(f"Prompt å‰ 200 å­—: {formatted_prompt[:200]}...")
        
        # === 4. Tokenize ===
        inputs = self._tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # ç§»å‹•åˆ°æ­£ç¢ºçš„è¨­å‚™
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # === 5. ç”Ÿæˆå›æ‡‰ ===
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    use_cache=True,
                    eos_token_id=self.terminators,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True if temperature > 0 else False
                )
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¤±æ•—: {e}", exc_info=True)
            return f"ç³»çµ±éŒ¯èª¤: ç”Ÿæˆå¤±æ•— ({str(e)})"
        
        # === 6. è§£ç¢¼èˆ‡åˆ‡å‰² ===
        full_text = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        try:
            # å˜—è©¦åˆ‡å‰² "### Output:" å¾Œçš„å…§å®¹
            if "### Output:" in full_text:
                generated_text = full_text.split("### Output:")[1].strip()
                
                # å¦‚æœæ¨¡å‹è‡ªå·±åˆç”Ÿæˆäº†ä¸‹ä¸€å€‹ Instruction,åˆ‡æ‰
                if "### Instruction:" in generated_text:
                    generated_text = generated_text.split("### Instruction:")[0].strip()
            else:
                # å¦‚æœæ²’æœ‰ "### Output:",å¯èƒ½æ˜¯ template ä¸åŒ
                # å˜—è©¦æ‰¾åˆ° input ä¹‹å¾Œçš„å…§å®¹
                generated_text = full_text
        
        except Exception as e:
            logger.warning(f"âš ï¸  æ–‡å­—åˆ‡å‰²å¤±æ•—,è¿”å›å®Œæ•´è¼¸å‡º: {e}")
            generated_text = full_text
        
        logger.debug(f"ç”Ÿæˆæ–‡å­—: {generated_text[:100]}...")
        
        return generated_text.strip()