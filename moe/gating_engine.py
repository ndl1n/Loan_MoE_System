import torch
import logging
import os
from typing import Dict, Tuple
from transformers import DistilBertTokenizer

from .config import (
    DEVICE, MODEL_PATH, STRUCT_DIM, MAX_LEN,
    ID2LABEL, STATUS_MAP, CONFIDENCE_THRESHOLD
)
from .model_arch import StateFirstGatingModel

logger = logging.getLogger(__name__)


class MoEGateKeeper:
    """
    MoE é–˜é–€å®ˆè¡› (æ ¹æ“šå¯¦éš›è¨“ç·´è³‡æ–™å„ªåŒ–)
    """
    
    def __init__(self):
        logger.info("ğŸ”„ åˆå§‹åŒ– MoE GateKeeper...")
        
        self.tokenizer = DistilBertTokenizer.from_pretrained(
            'distilbert-base-multilingual-cased'
        )
        
        self.model = StateFirstGatingModel(n_classes=3, struct_dim=STRUCT_DIM)
        self._load_weights()
        self.model.to(DEVICE)
        self.model.eval()
        
        logger.info("âœ… MoE GateKeeper æº–å‚™å°±ç·’!")

    def _load_weights(self):
        """è¼‰å…¥é è¨“ç·´æ¬Šé‡"""
        if os.path.exists(MODEL_PATH):
            self.model.load_state_dict(
                torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True)
            )
            logger.info(f"âœ… è¼‰å…¥æ¨¡å‹æ¬Šé‡: {MODEL_PATH}")
        else:
            raise FileNotFoundError(
                f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ¬Šé‡æª”: {MODEL_PATH}\n"
                f"è«‹ç¢ºèªå·²å°‡ .pth æª”æ¡ˆæ”¾å…¥ models è³‡æ–™å¤¾ã€‚"
            )

    def calculate_risk_score(self, profile: Dict) -> float:
        """
        è¨ˆç®—é¢¨éšªåˆ†æ•¸ (æ ¹æ“šè¨“ç·´è³‡æ–™å„ªåŒ–)
        
        è€ƒæ…®å› ç´ :
        1. è·æ¥­ç©©å®šæ€§
        2. æ”¶å…¥æ°´å¹³
        3. è²¸æ¬¾ç”¨é€”
        4. è² å‚µæ¯” (DTI)
        
        Returns:
            0.0 ~ 1.0 (0 = ä½é¢¨éšª, 1 = é«˜é¢¨éšª)
        """
        
        job = str(profile.get("job", "")).lower()
        income = profile.get("income", 0)
        purpose = str(profile.get("purpose", "")).lower()
        amount = profile.get("amount", 0)
        
        # === ç¶­åº¦ 1: è·æ¥­é¢¨éšª (40% æ¬Šé‡) ===
        job_risk = 0.5
        
        # æ ¹æ“šè¨“ç·´è³‡æ–™çš„è·æ¥­åˆ†å¸ƒ
        # é«˜é¢¨éšªè·æ¥­
        high_risk_jobs = [
            "è‡ªç”±æ¥­", "ç„¡æ¥­", "å¾…æ¥­", "è‡¨æ™‚å·¥", "æ‰“é›¶å·¥",
            "æ”¤è²©", "å®¶ç®¡", "å­¸ç”Ÿ", "å…¼è·"
        ]
        # ä½é¢¨éšªè·æ¥­
        low_risk_jobs = [
            "å…¬å‹™å“¡", "æ•™å¸«", "é†«å¸«", "å¾‹å¸«", "æœƒè¨ˆå¸«",
            "å·¥ç¨‹å¸«", "ä¸»ç®¡", "ç¶“ç†", "é‡‘è", "ç§‘æŠ€"
        ]
        
        for kw in high_risk_jobs:
            if kw in job:
                job_risk = 0.9
                break
        
        for kw in low_risk_jobs:
            if kw in job:
                job_risk = 0.1
                break
        
        # === ç¶­åº¦ 2: æ”¶å…¥é¢¨éšª (30% æ¬Šé‡) ===
        income_risk = 0.5
        
        if income > 0:
            if income < 30000:
                income_risk = 0.9
            elif income < 50000:
                income_risk = 0.6
            elif income < 70000:
                income_risk = 0.4
            elif income < 100000:
                income_risk = 0.2
            else:
                income_risk = 0.1
        
        # === ç¶­åº¦ 3: è²¸æ¬¾ç”¨é€”é¢¨éšª (20% æ¬Šé‡) ===
        purpose_risk = 0.5
        
        # ä½é¢¨éšªç”¨é€”
        low_risk_purposes = [
            "æˆ¿å±‹", "è³¼å±‹", "é ­æœŸæ¬¾", "æ•™è‚²", "é†«ç™‚", "å‰µæ¥­"
        ]
        # ä¸­é«˜é¢¨éšªç”¨é€”
        high_risk_purposes = [
            "æŠ•è³‡", "å‚µå‹™æ•´åˆ", "é€±è½‰", "å…¶ä»–"
        ]
        
        for kw in low_risk_purposes:
            if kw in purpose:
                purpose_risk = 0.2
                break
        
        for kw in high_risk_purposes:
            if kw in purpose:
                purpose_risk = 0.7
                break
        
        # === ç¶­åº¦ 4: è² å‚µæ¯”é¢¨éšª (10% æ¬Šé‡) ===
        dti_risk = 0.5
        
        if income > 0 and amount > 0:
            # å‡è¨­ 5 å¹´æœŸ,æœˆé‚„æ¬¾
            monthly_payment = amount / 60
            dti = monthly_payment / income
            
            if dti > 0.5:
                dti_risk = 1.0
            elif dti > 0.4:
                dti_risk = 0.8
            elif dti > 0.3:
                dti_risk = 0.6
            elif dti > 0.2:
                dti_risk = 0.3
            else:
                dti_risk = 0.1
        
        # === ç¶œåˆè©•åˆ† (åŠ æ¬Šå¹³å‡) ===
        risk_score = (
            job_risk * 0.4 +
            income_risk * 0.3 +
            purpose_risk * 0.2 +
            dti_risk * 0.1
        )
        
        logger.info(
            f"ğŸ“Š é¢¨éšªè©•åˆ†: {risk_score:.2f} "
            f"(è·æ¥­:{job_risk:.2f}, æ”¶å…¥:{income_risk:.2f}, "
            f"ç”¨é€”:{purpose_risk:.2f}, DTI:{dti_risk:.2f})"
        )
        
        return risk_score

    def predict(self, json_input: Dict) -> Tuple[str, float, str]:
        """
        é æ¸¬è·¯ç”±ç›®æ¨™
        
        Args:
            json_input: {
                "profile_state": {...},
                "verification_status": "unknown|pending|verified|mismatch",
                "user_query": "ç•¶å‰ä½¿ç”¨è€…å•é¡Œ"
            }
        
        Returns:
            (expert, confidence, reason)
        """
        
        profile = json_input.get("profile_state", {})
        status_str = json_input.get("verification_status", "unknown")
        text = json_input.get("user_query", "")
        
        # è¨ˆç®—é¢¨éšªåˆ†æ•¸
        risk_score = self.calculate_risk_score(profile)

        # ==========================
        # 1. é‚è¼¯è­·æ¬„å±¤ (Guardrails)
        # ==========================
        
        # [A] è³‡æ–™ä¸å®Œæ•´: unknown â†’ LDE
        if status_str == "unknown":
            logger.info("ğŸ›¡ï¸  Guardrail: è³‡æ–™æœªå®Œæˆ â†’ LDE")
            return "LDE", 1.0, "Guardrail: Incomplete Data (unknown status)"
        
        # [B] ç¼ºå°‘å¿…è¦æ¬„ä½: â†’ LDE
        # æ ¹æ“šè¨“ç·´è³‡æ–™,name å¿…é ˆæœ‰ (id å¯ä»¥ null)
        if not profile.get("name"):
            logger.info("ğŸ›¡ï¸  Guardrail: ç¼ºå°‘å§“å â†’ LDE")
            return "LDE", 1.0, "Guardrail: Missing Name"
        
        # [C] å·²é©—è­‰: verified â†’ FRE (é€²è¡Œé¢¨éšªè©•ä¼°)
        if status_str == "verified":
            logger.info("ğŸ›¡ï¸  Guardrail: å·²é©—è­‰ â†’ FRE")
            return "FRE", 1.0, "Guardrail: Verified Status â†’ Risk Assessment"
        
        # [D] æ¬„ä½ä¸ç¬¦: mismatch â†’ LDE (è®“å°ˆå“¡è™•ç†)
        if status_str == "mismatch":
            logger.info("ğŸ›¡ï¸  Guardrail: è³‡æ–™ä¸ç¬¦ â†’ LDE")
            return "LDE", 1.0, "Guardrail: Data Mismatch â†’ Agent Review"
        
        # [E] æŠ€è¡“å•é¡Œ: â†’ DVE
        tech_keywords = [
            "ç³»çµ±", "éŒ¯èª¤", "ç„¡æ³•", "bug", "æ•…éšœ", "ç•°å¸¸",
            "è£œä»¶", "é©—è­‰", "ç¢ºèª", "è³‡æ–™"
        ]
        if any(kw in text for kw in tech_keywords):
            logger.info("ğŸ›¡ï¸  Guardrail: æŠ€è¡“/è£œä»¶å•é¡Œ â†’ DVE")
            return "DVE", 0.95, "Guardrail: Technical/Verification Issue"
        
        # [F] pending ç‹€æ…‹ä¸‹çš„è·¯ç”±é‚è¼¯
        if status_str == "pending":
            # pending + é«˜é¢¨éšª â†’ DVE (å…ˆåš´æ ¼é©—è­‰)
            if risk_score >= 0.7:
                logger.info("ğŸ›¡ï¸  Guardrail: Pending + é«˜é¢¨éšª â†’ DVE")
                return "DVE", 0.90, "Guardrail: High Risk Verification"
            
            # pending + æ¥µä½é¢¨éšª â†’ DVE (ä½†å¯èƒ½å¿«é€Ÿé€šé)
            # æ³¨æ„: æ ¹æ“šè¨“ç·´è³‡æ–™,pending é€šå¸¸æœƒåˆ° DVE
            if risk_score <= 0.3:
                logger.info("ğŸ›¡ï¸  Guardrail: Pending + ä½é¢¨éšª â†’ DVE")
                return "DVE", 0.85, "Guardrail: Low Risk Quick Verification"
        
        # [G] é¡åº¦ç›¸é—œå•é¡Œ: verified â†’ FRE
        quota_keywords = ["é¡åº¦", "ç”³è¦†", "é‡‘é¡", "å¤šå°‘éŒ¢", "å¯ä»¥è²¸"]
        if any(kw in text for kw in quota_keywords) and status_str == "verified":
            logger.info("ğŸ›¡ï¸  Guardrail: é¡åº¦å•é¡Œ â†’ FRE")
            return "FRE", 0.95, "Guardrail: Quota/Amount Inquiry"

        # ==========================
        # 2. AI æ¨ç†å±¤
        # ==========================
        logger.info("ğŸ¤– é€²å…¥ AI æ¨ç†å±¤...")
        return self._ai_inference(text, profile, status_str, risk_score)

    def _ai_inference(
        self,
        text: str,
        profile: Dict,
        status_str: str,
        risk_score: float
    ) -> tuple:
        """
        AI æ¨¡å‹æ¨ç†
        """
        
        try:
            # === æº–å‚™æ–‡å­—ç‰¹å¾µ ===
            encoding = self.tokenizer.encode_plus(
                text,
                max_length=MAX_LEN,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            
            # === æº–å‚™çµæ§‹ç‰¹å¾µ (7 ç¶­) ===
            # æ ¹æ“šè¨“ç·´æ™‚çš„ç‰¹å¾µå·¥ç¨‹
            
            # 1. æ¬„ä½å­˜åœ¨æ€§
            has_id = 1.0 if profile.get("id") else 0.0
            has_name = 1.0 if profile.get("name") else 0.0
            has_job = 1.0 if profile.get("job") else 0.0
            has_income = 1.0 if profile.get("income") else 0.0
            
            # 2. ç‹€æ…‹å€¼
            status_val = STATUS_MAP.get(status_str, 0)
            
            # 3. è³‡æ–™å®Œæ•´åº¦
            all_fields = ["name", "id", "job", "income", "purpose", "amount"]
            filled = sum(1 for f in all_fields if profile.get(f) is not None)
            sparsity = filled / len(all_fields)
            
            # 4. é¢¨éšªåˆ†æ•¸
            # å·²ç¶“è¨ˆç®—å¥½äº†
            
            # çµ„è£ç‰¹å¾µå‘é‡
            struct_features = torch.tensor([
                has_id,
                has_name,
                has_job,
                has_income,
                status_val / 4.0,  # æ­£è¦åŒ– (0~4 â†’ 0~1)
                sparsity,
                risk_score
            ], dtype=torch.float).unsqueeze(0).to(DEVICE)
            
            logger.debug(f"çµæ§‹ç‰¹å¾µ: {struct_features.cpu().numpy()}")
            
            # === æ¨¡å‹æ¨ç† ===
            input_ids = encoding['input_ids'].to(DEVICE)
            attention_mask = encoding['attention_mask'].to(DEVICE)

            with torch.no_grad():
                outputs = self.model(input_ids, attention_mask, struct_features)
                probs = torch.nn.functional.softmax(outputs, dim=1)
                pred_idx = torch.argmax(probs, dim=1).item()
                confidence = probs[0][pred_idx].item()
            
            expert = ID2LABEL[pred_idx]
            
            logger.info(
                f"ğŸ¯ AI æ¨ç†: {expert} (ä¿¡å¿ƒåº¦: {confidence:.2f}), "
                f"æ©Ÿç‡åˆ†å¸ƒ: {probs.cpu().numpy()}"
            )
            
            # === ä¿¡å¿ƒåº¦æª¢æŸ¥ ===
            if confidence < CONFIDENCE_THRESHOLD:
                logger.warning(
                    f"âš ï¸  ä¿¡å¿ƒåº¦éä½ ({confidence:.2f}), ä½¿ç”¨è¦å‰‡å¼ Fallback"
                )
                return self._rule_based_fallback(profile, status_str, risk_score)
            
            return expert, confidence, "AI Model Inference"
        
        except Exception as e:
            logger.error(f"âŒ AI æ¨ç†å¤±æ•—: {e}", exc_info=True)
            return self._rule_based_fallback(profile, status_str, risk_score)
    
    def _rule_based_fallback(
        self,
        profile: Dict,
        status_str: str,
        risk_score: float
    ) -> Tuple[str, float, str]:
        """
        è¦å‰‡å¼ Fallback
        """
        
        logger.info("ğŸ”§ ä½¿ç”¨è¦å‰‡å¼ Fallback")
        
        if status_str == "unknown":
            return "LDE", 0.75, "Rule Fallback: Unknown â†’ LDE"
        elif status_str == "pending":
            return "DVE", 0.75, "Rule Fallback: Pending â†’ DVE"
        elif status_str == "verified":
            return "FRE", 0.75, "Rule Fallback: Verified â†’ FRE"
        elif status_str == "mismatch":
            return "LDE", 0.75, "Rule Fallback: Mismatch â†’ LDE"
        else:
            return "LDE", 0.5, "Rule Fallback: Default â†’ LDE"