import json
import torch
from transformers import TextStreamer
from ..config import DVE_ADAPTER_PATH, DVE_PROMPT_TEMPLATE, DVE_INSTRUCTION, DEVICE
from .base import BaseExpert

class DVE_Expert(BaseExpert):
    """
    DVE: è³‡æ–™æŸ¥æ ¸å°ˆå®¶ (Schema Fix Version)
    """
    def process(self, task_data, history=[]):
        query = task_data.get("user_query", "")
        profile = task_data.get("profile_state", {})
        
        # 1. æŠ€è¡“éšœç¤™æ””æˆª (Rule-based)
        tech_keywords = ["å‚³ä¸ä¸Š", "å¤±æ•—", "æ ¼å¼éŒ¯èª¤", "å¤ªæ…¢", "ç•¶æ©Ÿ", "ç„¡æ³•"]
        if any(k in query for k in tech_keywords):
            return {
                "expert": "DVE (Tech Support)",
                "response": "åµæ¸¬åˆ°æŠ€è¡“å•é¡Œã€‚è«‹ç¢ºèªåœ–ç‰‡æ ¼å¼ç‚º JPG/PNG ä¸”å°æ–¼ 5MBã€‚",
                "next_step": "ç­‰å¾…æŠ€è¡“æ’é™¤"
            }

        # 2. æº–å‚™æ¯”å°è³‡æ–™
        # ç‚ºäº†ä¸è®“æ¨¡å‹ç•¶æ©Ÿï¼Œæˆ‘å€‘å¿…é ˆã€Œæ¹Šé½Šã€è¨“ç·´è³‡æ–™è£¡çš„æ‰€æœ‰æ¬„ä½
        print("ğŸ›¡ï¸ DVE å•Ÿå‹• AI æŸ¥æ ¸æ¨¡å¼ (Schema Aligned)...")
        
        # [æ¨¡æ“¬ RAG]ï¼šé€™è£¡è¦æŠŠæ‰€æœ‰è¨“ç·´è³‡æ–™æœ‰çš„ key éƒ½è£œä¸Šï¼Œæ²’æœ‰çš„å°±å¡« "ç„¡"
        mock_rag_context = {
            "æª”æ¡ˆä¸­ç´€éŒ„è·æ¥­": "å…¬ç«‹é«˜ä¸­æ•™å¸«",   # æ¨¡æ“¬æ­·å²è³‡æ–™
            "ä¸Šæ¬¡è²¸æ¬¾è³‡é‡‘ç”¨é€”": "æˆ¿å±‹ä¿®ç¹•",     # (è£œ)
            "æª”æ¡ˆä¸­è¯çµ¡é›»è©±": "0920-987-654",
            "æ­·å²é•ç´„ç´€éŒ„": "ç„¡",
            "æª”æ¡ˆä¸­æœå‹™å…¬å¸åç¨±": "XXå¸‚ç«‹é«˜ä¸­",
            "æª”æ¡ˆä¸­å¹´è–ª/æœˆè–ª": "60000",
            "ä¿¡ç”¨å ±å‘ŠæŸ¥è©¢æ¬¡æ•¸": "1",           # (è£œ)
            "åœ°å€è®Šå‹•æ¬¡æ•¸": "0"                # (è£œ)
        }
        
        # [çµ„å»º Input]ï¼šé€™è£¡çš„ Key å¿…é ˆè·Ÿè¨“ç·´è³‡æ–™ä¸€æ¨¡ä¸€æ¨£ï¼
        dve_input_data = {
            "æ ¸å¿ƒè­˜åˆ¥è³‡è¨Š": {
                "ç”³è«‹äººå§“å": profile.get("name", "æ¸¬è©¦äººå“¡"),
                "èº«åˆ†è­‰å­—è™Ÿ": profile.get("id", "A123456789")
            },
            "æœ€æ–°å£è¿°è³‡è¨Š (Query) æ“·å–": {
                "è·æ¥­": profile.get("job", "å¾…æ¥­ä¸­"),  # å¾ Profile æ‹¿ï¼Œæ²’æœ‰å°±å¡«é è¨­
                "è³‡é‡‘ç”¨é€”": "é€±è½‰é‡‘",                  # (å¯«æ­») æš«æ™‚å¡«å…¥ï¼Œä¹‹å¾Œå¯å¾å°è©±åˆ†æ
                "è¯çµ¡é›»è©±": "0912-345-678",            # (å¯«æ­») 
                "æœå‹™å…¬å¸åç¨±": "æœªæä¾›",              # (å¯«æ­»)
                "æœˆè–ª": str(profile.get("income", "0"))
            },
            "RAG æª¢ç´¢çš„æ­·å²æ•¸æ“š (Context) æ“·å–": mock_rag_context
        }
        
        input_json_str = json.dumps(dve_input_data, ensure_ascii=False)

        # 3. å‘¼å« LLM (åŠ å…¥ Streamer ç›£æ§)
        # è¨­å®š Streamerï¼Œè®“å®ƒå³æ™‚å°å‡ºæ–‡å­—ï¼Œé€™æ¨£ä½ å°±çŸ¥é“å®ƒæœ‰æ²’æœ‰åœ¨è·‘
        streamer = TextStreamer(self.llm._tokenizer, skip_prompt=True)
        
        print(f"ğŸŒŠ Input JSON å·²æ§‹å»ºï¼Œé•·åº¦: {len(input_json_str)} chars")
        print("ğŸŒŠ é–‹å§‹ç”Ÿæˆ (Stream Mode)... è«‹çœ‹ä¸‹æ–¹è¼¸å‡º ğŸ‘‡")

        # ä½¿ç”¨ llm_utils çš„åº•å±¤ generate (ç‚ºäº†å‚³å…¥ streamer)
        # é€™è£¡æˆ‘å€‘ç¨å¾®ç¹é get_expert_response çš„å°è£ï¼Œç›´æ¥èª¿ç”¨ä»¥ç¢ºä¿èƒ½çœ‹åˆ° Stream
        
        model = self.llm._base_model
        tokenizer = self.llm._tokenizer
        
        # è¼‰å…¥ Adapter
        from peft import PeftModel
        model = PeftModel.from_pretrained(model, DVE_ADAPTER_PATH)
        model.eval()

        # æ ¼å¼åŒ– Prompt
        prompt = DVE_PROMPT_TEMPLATE.format(DVE_INSTRUCTION, input_json_str)
        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                streamer=streamer,            # <--- é—œéµï¼šå³æ™‚é¡¯ç¤º
                max_new_tokens=512,
                temperature=0.1,              # DVE éœ€è¦ä½æº«
                repetition_penalty=1.2,       # é˜²æ­¢é¬¼æ‰“ç‰†
                eos_token_id=tokenizer.eos_token_id
            )
        
        # 4. è§£æèˆ‡ç­–ç•¥åˆ†æµ (ä¿®æ­£ç‰ˆï¼šå¼·åŠ›é˜²é¬¼æ‰“ç‰†)
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=False) # æ”¹æˆ False ä»¥ä¾¿æˆ‘å€‘åµæ¸¬ç‰¹æ®Šç¬¦è™Ÿ
        
        try:
            # === [æ–°å¢] å¼·åŠ›åˆ‡å‰²é‚è¼¯ ===
            # 1. å¦‚æœå‡ºç¾çµæŸç¬¦è™Ÿï¼Œç›´æ¥åˆ‡æ–·
            if "<|end_of_text|>" in full_text:
                full_text = full_text.split("<|end_of_text|>")[0]
            
            # 2. å¦‚æœå‡ºç¾ä¸‹ä¸€å€‹æŒ‡ä»¤çš„é–‹é ­ï¼Œç›´æ¥åˆ‡æ–·
            if "<|begin_of_text|>" in full_text:
                full_text = full_text.split("<|begin_of_text|>")[1] # å–ä¸­é–“é‚£æ®µ
                if "<|begin_of_text|>" in full_text: # å¦‚æœé‚„æœ‰ç¬¬äºŒå€‹
                     full_text = full_text.split("<|begin_of_text|>")[0]

            # 3. æŠ“å– Output ä¹‹å¾Œçš„ JSON
            if "### Output:" in full_text:
                generated_text = full_text.split("### Output:")[1].strip()
            else:
                generated_text = full_text

            # 4. JSON æ¸…æ´— (åªæŠ“å–ç¬¬ä¸€å€‹å®Œæ•´çš„ {} ç‰©ä»¶)
            # é€™æ˜¯é˜²æ­¢å¾Œé¢é‡è¤‡å‡ºç¾ {"æ ¸å¯¦ç‹€æ…‹"...} çš„é—œéµ
            start_idx = generated_text.find("{")
            
            # æˆ‘å€‘åˆ©ç”¨è¨ˆæ•¸å™¨ä¾†æ‰¾å°æ‡‰çš„çµæŸæ‹¬è™Ÿï¼Œè€Œä¸æ˜¯ç”¨ rfind
            # é€™æ¨£å°±ç®—å¾Œé¢æœ‰é‡è¤‡çš„ JSONï¼Œæˆ‘å€‘ä¹ŸåªæœƒæŠ“ç¬¬ä¸€å€‹
            if start_idx != -1:
                brace_count = 0
                end_idx = -1
                for i, char in enumerate(generated_text[start_idx:], start=start_idx):
                    if char == "{":
                        brace_count += 1
                    elif char == "}":
                        brace_count -= 1
                        if brace_count == 0:
                            end_idx = i
                            break
                
                if end_idx != -1:
                    generated_text = generated_text[start_idx : end_idx+1]
                else:
                    # è¬ä¸€æ²’æ‰¾åˆ°çµå°¾ï¼Œå°±ç”¨èˆŠæ–¹æ³•å…œåº•
                    generated_text = generated_text[start_idx : generated_text.rfind("}")+1]

            print(f"\nğŸ” æ“·å–åˆ°çš„æœ€çµ‚ JSON: {generated_text[:100]}...") # Debug ç”¨

            report = json.loads(generated_text)
            
            # --- è®€å–çµæœ (ä¿æŒä¸è®Š) ---
            risk_level = report.get("é¢¨éšªæ¨™è¨˜", "MEDIUM")
            
            if risk_level == "LOW":
                user_res = "è³‡æ–™é©—è­‰ç„¡èª¤ï¼Œæ­£åœ¨ç‚ºæ‚¨é€²è¡Œè©¦ç®—ã€‚"
                next_step = "TRANSFER_TO_FRE"
            elif risk_level == "HIGH":
                user_res = "ç³»çµ±åµæ¸¬åˆ°æ‚¨çš„è³‡æ–™èˆ‡ç´€éŒ„æœ‰å‡ºå…¥ï¼Œè«‹èªªæ˜ç›®å‰ç‹€æ³ã€‚"
                next_step = "FORCE_LDE_CLARIFY"
            else:
                user_res = "è³‡æ–™å·²å—ç†ï¼Œå°‡è½‰ç”±äººå·¥è¦†æ ¸ã€‚"
                next_step = "TRANSFER_TO_FRE"

            return {
                "expert": f"DVE ({risk_level})",
                "response": user_res,
                "dve_raw_report": report,
                "next_step": next_step
            }

        except Exception as e:
            print(f"\nâŒ DVE è§£æå¤±æ•—: {e}")
            # å¦‚æœè§£æå¤±æ•—ï¼Œå°å‡ºåŸæ–‡è®“æˆ‘å€‘é™¤éŒ¯
            # print(f"Raw Text: {full_text}") 
            return {
                "expert": "DVE (Error)",
                "response": "ç³»çµ±å¿™ç¢Œä¸­ï¼Œè«‹ç¨å¾Œã€‚",
                "next_step": "HUMAN_HANDOVER"
            }