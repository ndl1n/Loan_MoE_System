import json
import torch
import time
from datetime import datetime
from transformers import TextStreamer
from peft import PeftModel
from ..config import FRE_ADAPTER_PATH, FRE_PROMPT_TEMPLATE, FRE_INSTRUCTION, DEVICE
from .base import BaseExpert

class FRE_Expert(BaseExpert):
    """
    FRE: æœ€çµ‚é¢¨æ§å°ˆå®¶ (Streamer + Schema Matching + Safety Guard)
    """
    
    def process(self, task_data, history=[]):
        profile = task_data.get("profile_state", {})
        dve_result = task_data.get("dve_result", {})
        
        # --- 1. è¼”åŠ©å‡½æ•¸ï¼šè™•ç† Null èˆ‡å‹åˆ¥è½‰æ› ---
        def safe_int(val, default=0):
            try:
                if val in [None, "null", "è³‡æ–™ä¸è¶³", "", "None"]: return default
                return int(float(val))
            except: return default

        def safe_str(val, default="è³‡æ–™ä¸è¶³"):
            if val in [None, "null", "", "None"]: return default
            return str(val)

        # --- 2. æº–å‚™æ•¸æ“š & æ•¸å­¸è¨ˆç®— (Python Layer) ---
        p_income = safe_int(profile.get("income"))
        p_amount = safe_int(profile.get("amount"))
        p_job = safe_str(profile.get("job"))
        
        # å‡è¨­æ­·å²è–ªè³‡ (è‹¥ç„¡å‰‡ç”¨å£è¿°)
        h_income = p_income if p_income > 0 else 60000 
        calc_base_income = p_income if p_income > 0 else h_income
        
        # è¨ˆç®— DBR
        if p_amount > 0:
            monthly_pay = int((p_amount * 1.03) / 84) 
        else:
            monthly_pay = 0
        
        dbr = (monthly_pay / calc_base_income * 100) if calc_base_income > 0 else 0
        
        # æ¨¡æ“¬ä¿¡ç”¨è©•åˆ† (ç°¡å–®è¦å‰‡ï¼šè–ªè³‡é«˜åˆ†å°±é«˜)
        credit_score = 700 if calc_base_income > 40000 else 600

        # --- 3. æ§‹å»º Input JSON (Schema å°é½Šè¨“ç·´è³‡æ–™) ---
        current_time = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        
        fre_input_data = {
            "caseId": f"CASE_{int(time.time())}",
            "creationDate": current_time,
            "scenarioType": "REAL_TIME_INFERENCE",
            "customerIdentity": {
                "èº«åˆ†è­‰å­—è™Ÿ": safe_str(profile.get("id"), "UNKNOWN"),
                "ç”³è«‹äººå§“å": safe_str(profile.get("name"), "Guest")
            },
            "applicationData": {
                "ç”³è«‹é‡‘é¡": p_amount,
                "ç”³è«‹ç”¨é€”_å®˜æ–¹": "é€±è½‰é‡‘"
            },
            "creditReportData": {
                "ç³»çµ±åŸå§‹ä¿¡ç”¨è©•åˆ†": credit_score,
                "ç¾æœ‰ç¸½è² å‚µé‡‘é¡": 0,
                "æ­·å²é•ç´„ç´€éŒ„": "ç„¡" if credit_score >= 650 else "æœ‰",
                "ä¿¡ç”¨å ±å‘ŠæŸ¥è©¢æ¬¡æ•¸_è¿‘3æœˆ": 1
            },
            "providedData": {
                "å£è¿°æœˆè–ª": profile.get("income"), # ä¿ç•™ None è®“æ¨¡å‹çœ‹
                "å£è¿°è·æ¥­": p_job,
                "å£è¿°å…¬å¸åç¨±": safe_str(profile.get("company")),
                "å£è¿°è¯çµ¡é›»è©±": "09xx-xxx-xxx",
                "å£è¿°è³‡é‡‘ç”¨é€”": "é€±è½‰é‡‘"
            },
            "historicalData": {
                "æ­·å²æœˆè–ª": h_income, 
                "æ­·å²è·æ¥­": "è³‡æ–™åº«ç´€éŒ„"
            },
            "system_hint": {
                "dve_risk_label": dve_result.get("risk_level", "LOW"),
                "calculated_dbr": f"{dbr:.1f}%"
            }
        }
        
        input_json_str = json.dumps(fre_input_data, ensure_ascii=False)
        print(f"ğŸ’° FRE Input æ§‹å»ºå®Œæˆ (DBR: {dbr:.1f}%, Score: {credit_score})")

        # --- 4. å‘¼å« LLM (Stream Mode) ---
        print("ğŸŒŠ é–‹å§‹ç”Ÿæˆæ±ºç­– (Stream Mode)... ğŸ‘‡")
        
        # é€™è£¡ä¸ä½¿ç”¨ get_expert_responseï¼Œè€Œæ˜¯ç›´æ¥èª¿ç”¨ä»¥å•Ÿç”¨ Streamer
        streamer = TextStreamer(self.llm._tokenizer, skip_prompt=True)
        model = self.llm._base_model
        tokenizer = self.llm._tokenizer
        
        # æ›è¼‰ Adapter
        model = PeftModel.from_pretrained(model, FRE_ADAPTER_PATH)
        model.eval()

        # æº–å‚™ Prompt
        prompt = FRE_PROMPT_TEMPLATE.format(FRE_INSTRUCTION, input_json_str)
        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                streamer=streamer,            # <--- é—œéµï¼šå•Ÿç”¨ç›´æ’­
                max_new_tokens=512,
                temperature=0.1,
                repetition_penalty=1.2,       # <--- é—œéµï¼šé˜²æ­¢é‡è¤‡
                eos_token_id=tokenizer.eos_token_id
            )

        # --- 5. è§£æèˆ‡å¼·åŠ›é‚è¼¯çŸ¯æ­£ (Safety Guard) ---
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        try:
            # åˆ‡å‰²é¬¼æ‰“ç‰†æ–‡å­—
            if "<|end_of_text|>" in full_text: full_text = full_text.split("<|end_of_text|>")[0]
            if "### Output:" in full_text: generated_text = full_text.split("### Output:")[1].strip()
            else: generated_text = full_text

            # JSON æ¸…æ´— (åªæŠ“å–ç¬¬ä¸€å€‹å®Œæ•´çš„ {} ç‰©ä»¶)
            if "{" in generated_text:
                generated_text = generated_text[generated_text.find("{"):generated_text.rfind("}")+1]

            report = json.loads(generated_text)
            
            # å–å¾—æ±ºç­–
            # å˜—è©¦é©æ‡‰ä¸åŒçš„ JSON çµæ§‹ (æœ‰äº›è¨“ç·´è³‡æ–™æœ‰ "æœ€çµ‚æ±ºç­–å ±å‘Š" å±¤ç´šï¼Œæœ‰äº›ç›´æ¥æ˜¯æ ¹ç›®éŒ„)
            final_block = report.get("æœ€çµ‚æ±ºç­–å ±å‘Š", report)
            decision = final_block.get("æœ€çµ‚æ±ºç­–") or report.get("æœ€çµ‚æ±ºç­–") or "è½‰ä»‹å¯©æ ¸_ESCALATE"
            
            # === å®‰å…¨é– (Logic Override) ===
            override_msg = ""
            
            # 1. é—œéµè³‡æ–™ç¼ºå¤± -> å¼·åˆ¶è½‰äººå·¥
            missing_critical = (p_income == 0) or (p_job == "è³‡æ–™ä¸è¶³")
            if missing_critical and ("PASS" in decision or "æ ¸å‡†" in decision):
                decision = "è½‰ä»‹å¯©æ ¸_ESCALATE"
                override_msg = "(ç³»çµ±ä¿®æ­£: é—œéµè³‡æ–™ç¼ºå¤±)"
                print("âš ï¸ FRE Guard: æ””æˆªåˆ°è³‡æ–™ç¼ºå¤±")

            # 2. DBR éé«˜ -> å¼·åˆ¶æ‹’çµ•
            if dbr > 60 and ("PASS" in decision):
                decision = "æ‹’çµ•_REJECT"
                override_msg = f"(ç³»çµ±ä¿®æ­£: DBR {dbr:.1f}% éé«˜)"
                print("âš ï¸ FRE Guard: æ””æˆªåˆ°é«˜è² å‚µæ¯”")
            
            # 3. ä¿¡ç”¨åˆ†éä½ -> å¼·åˆ¶æ‹’çµ•
            if credit_score < 650 and ("PASS" in decision):
                decision = "æ‹’çµ•_REJECT"
                override_msg = "(ç³»çµ±ä¿®æ­£: ä¿¡ç”¨åˆ†ä¸è¶³)"

            # --- å›æ‡‰ç”Ÿæˆ ---
            if "PASS" in decision or "æ ¸å‡†" in decision:
                user_msg = f"æ­å–œï¼æ‚¨çš„ä¿¡ç”¨è©•åˆ† ({credit_score}åˆ†) ç¬¦åˆæ¨™æº–ã€‚\nåˆå¯©é¡åº¦: {p_amount:,} å…ƒ"
                next_step = "CASE_CLOSED_SUCCESS"
            elif "REJECT" in decision or "æ‹’çµ•" in decision:
                user_msg = "æ„Ÿè¬ç”³è«‹ã€‚ç¶“ç¶œåˆè©•ä¼°ï¼Œæš«æ™‚ç„¡æ³•æ ¸è²¸ã€‚"
                next_step = "CASE_CLOSED_REJECT"
            else:
                user_msg = "ç”³è«‹å·²å—ç†ï¼Œå°‡è½‰ç”±äººå·¥è¦†æ ¸ã€‚"
                next_step = "HUMAN_HANDOVER"

            return {
                "expert": f"FRE ({decision}) {override_msg}",
                "response": user_msg,
                "fre_raw_report": report,
                "financial_metrics": {"dbr": dbr, "score": credit_score},
                "next_step": next_step
            }

        except Exception as e:
            print(f"\nâŒ FRE è§£æå¤±æ•—: {e}")
            return {
                "expert": "FRE (Error)",
                "response": "ç³»çµ±å¿™ç¢Œä¸­ã€‚",
                "next_step": "HUMAN_HANDOVER"
            }