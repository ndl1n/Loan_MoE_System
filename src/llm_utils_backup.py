import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from .config import BASE_MODEL_PATH, DEVICE, LDE_PROMPT_TEMPLATE

class LocalLLMManager:
    _instance = None
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        print(f"ğŸ”„ [WSL2] Loading Base Model: {BASE_MODEL_PATH}...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        self._tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)
        self._base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            quantization_config=bnb_config,
            device_map=DEVICE,
            trust_remote_code=True
        )
        self.terminators = [
            self._tokenizer.eos_token_id,
            self._tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        print("âœ… Base Model Loaded.")

    def get_expert_response(self, adapter_path, instruction, user_input, 
                          max_new_tokens=256, temperature=0.3, top_p=0.9, template=None):
        """
        é€šç”¨å°ˆå®¶æ¨è«–å‡½æ•¸
        """
        if not os.path.exists(os.path.join(adapter_path, "adapter_model.safetensors")):
             return "ç³»çµ±éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ¬Šé‡æª”"
        
        # 1. æ›è¼‰ Adapter
        model = PeftModel.from_pretrained(self._base_model, adapter_path)
        model.eval()
        
        # 2. æ ¼å¼åŒ–è¼¸å…¥
        # å¦‚æœæœ‰å‚³å…¥ç‰¹å®šçš„ template (å¦‚ DVE_PROMPT_TEMPLATE) å°±ç”¨å®ƒï¼Œå¦å‰‡ç”¨é è¨­çš„
        if template:
            formatted_prompt = template.format(instruction, user_input)
        else:
            # é è¨­ fallback (å¦‚æœæ²’å‚³ template)
            formatted_prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{user_input}\n\n### Output:\n"
        
        inputs = self._tokenizer(formatted_prompt, return_tensors="pt").to(DEVICE)
        
        # 3. ç”Ÿæˆå›æ‡‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                use_cache=True,
                eos_token_id=self.terminators,
                temperature=temperature,  # å‹•æ…‹èª¿æ•´
                top_p=top_p
            )
        
        # 4. è§£ç¢¼èˆ‡åˆ‡å‰²
        full_text = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        try:
            generated_text = full_text.split("### Output:")[1].strip()
            # DVE ä¿®æ­£ï¼šæœ‰æ™‚å€™æ¨¡å‹æœƒè‡ªå·±åå‡ºä¸‹ä¸€å€‹ ### Instructionï¼Œè¦åˆ‡æ‰
            if "### Instruction:" in generated_text:
                generated_text = generated_text.split("### Instruction:")[0].strip()
        except IndexError:
            generated_text = full_text
            
        return generated_text